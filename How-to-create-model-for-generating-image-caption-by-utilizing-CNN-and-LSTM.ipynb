{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ndLvRkOy1g4T",
        "outputId": "9a4dd3c2-e619-4828-e941-32e55adb7b69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlEJ9ovuQjWo"
      },
      "source": [
        "#Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41i3LZStPShL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import string\n",
        "from PIL import Image\n",
        "from pickle import dump, load\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "\n",
        "\n",
        "from keras.applications.xception import Xception, preprocess_input\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
        "from keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import add\n",
        "from keras.preprocessing.image import load_img, img_to_array\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xahf6eFeQtNk"
      },
      "source": [
        "#Getting and performing data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xvO45E3Lg-6"
      },
      "outputs": [],
      "source": [
        "# Loading a text file into memory\n",
        "def load_text_file(filename):\n",
        "    # Opening the file as read only\n",
        "    file = open(filename, 'r')\n",
        "    text_data = file.read()\n",
        "    file.close()\n",
        "    return text_data\n",
        "\n",
        "# Retrieve all images and their captions from a file\n",
        "def extract_image_captions(filename):\n",
        "    # Load the content of the file\n",
        "    file_content = load_text_file(filename)\n",
        "    # Split the content into individual captions\n",
        "    captions = file_content.split('\\n')\n",
        "    lenght_captions = round(len(captions) / 2)\n",
        "    image_captions = {}\n",
        "\n",
        "    # Process each caption and link it to the respective image\n",
        "    for caption in captions[:lenght_captions]:\n",
        "        img, caption = caption.split('\\t')\n",
        "\n",
        "        # Check if the image is already in the image_captions dictionary\n",
        "        if img[:-2] not in image_captions:\n",
        "            image_captions[img[:-2]] = [caption]\n",
        "        else:\n",
        "            image_captions[img[:-2]].append(caption)\n",
        "\n",
        "    # Return a dictionary with images as keys and their associated captions as values\n",
        "    return image_captions\n",
        "\n",
        "# Data cleaning: Convert to lowercase, remove punctuation, and filter out words with numbers\n",
        "def clean_text_data(captions):\n",
        "    #creates a translation table that contains mapping information\n",
        "    #of several characters.\n",
        "    translation_table = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Iterate through each image and its associated captions\n",
        "    for image, image_captions in captions.items():\n",
        "        for i, image_caption in enumerate(image_captions):\n",
        "            # Replace hyphens with spaces\n",
        "            image_caption = image_caption.replace(\"-\", \" \")\n",
        "            words = image_caption.split()\n",
        "\n",
        "            # Convert words to lowercase\n",
        "            words = [word.lower() for word in words]\n",
        "\n",
        "            # Remove punctuation from each word\n",
        "            #the translate() function to replace these characters with their\n",
        "            #corresponding characters in the table.\n",
        "            words = [word.translate(translation_table) for word in words]\n",
        "\n",
        "            # Remove words with a single character (e.g., 'a') and words with numbers\n",
        "            words = [word for word in words if len(word) > 1 and word.isalpha()]\n",
        "\n",
        "            # Convert the cleaned words back to a string\n",
        "            image_captions = ' '.join(words)\n",
        "            captions[image][i] = image_captions\n",
        "\n",
        "    return captions\n",
        "\n",
        "# Create a vocabulary containing all unique words from descriptions\n",
        "def build_text_vocabulary(descriptions):\n",
        "    # Initialize an empty set to store unique words\n",
        "    vocabulary = set()\n",
        "\n",
        "    # Iterate through each image key and its associated descriptions\n",
        "    for image_key in descriptions.keys():\n",
        "        # Update the vocabulary with words from each description\n",
        "        [vocabulary.update(description.split()) for description in descriptions[image_key]]\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        " # Store all descriptions in a single file\n",
        "def store_descriptions_in_file(descriptions, filename):\n",
        "    lines = list()\n",
        "\n",
        "    # Iterate through each image key and its associated descriptions\n",
        "    for image_key, description_list in descriptions.items():\n",
        "        for description in description_list:\n",
        "            # Combine the image key and description with a tab separator\n",
        "            lines.append(image_key + '\\t' + description)\n",
        "\n",
        "    # Join the lines with newline characters to create the data\n",
        "    data = \"\\n\".join(lines)\n",
        "\n",
        "    # Open the file in write mode, write the data, and close the file\n",
        "    file = open(filename, \"w\")\n",
        "    file.write(data)\n",
        "    file.close()\n",
        "\n",
        "# Set the paths to the dataset folders in your project directory\n",
        "text_data_folder = \"/content/drive/MyDrive/DL/Flickr8k_text\"\n",
        "image_data_folder = \"/content/drive/MyDrive/DL/Flicker8k_Dataset\"\n",
        "\n",
        "# Define the file path for the text data\n",
        "text_data_file = text_data_folder + \"/\" + \"Flickr8k.token.txt\"\n",
        "\n",
        "# Load the file containing all data and create a dictionary mapping images to their captions\n",
        "image_captions = extract_image_captions(text_data_file)\n",
        "\n",
        "# Clean the descriptions\n",
        "cleaned_descriptions = clean_text_data(image_captions)\n",
        "\n",
        "# Build the vocabulary of unique words from the cleaned descriptions\n",
        "word_vocab = build_text_vocabulary(cleaned_descriptions)\n",
        "\n",
        "\n",
        "# Save each cleaned description to a file\n",
        "store_descriptions_in_file(cleaned_descriptions, \"cleaned_descriptions.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2OrMP-xQfnO"
      },
      "source": [
        "#Extracting the feature vector from all images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kk1rvBcpQN7R"
      },
      "outputs": [],
      "source": [
        "# Extract image features using the Xception model\n",
        "def extract_image_features(directory):\n",
        "    # Load the Xception model with the top layer removed and global average pooling\n",
        "    model = Xception(include_top=False, pooling='avg')\n",
        "    extracted_features = {}\n",
        "\n",
        "    # Iterate through the images in the specified directory\n",
        "    for image_filename in os.listdir(directory):\n",
        "        full_path = directory + \"/\" + image_filename\n",
        "        image = Image.open(full_path)\n",
        "        image = image.resize((299, 299))\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        # Preprocess the image data\n",
        "        image = image / 127.5\n",
        "        image = image - 1.0\n",
        "\n",
        "        # Extract image features using the model\n",
        "        feature_vector = model.predict(image)\n",
        "        extracted_features[image_filename] = feature_vector\n",
        "\n",
        "    return extracted_features\n",
        "\n",
        "# Extract 2048-dimensional feature vectors for images and save them to a file\n",
        "image_features = extract_image_features(image_data_folder)\n",
        "dump(image_features, open(\"image_features.p\", \"wb\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcK3KBUSU09B"
      },
      "source": [
        "#Loading dataset for Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZP05bDpnU3IX"
      },
      "outputs": [],
      "source": [
        "# Load data for training\n",
        "def load_training_data(filename):\n",
        "    # Load the list of photo filenames\n",
        "    file = load_text_file(filename)\n",
        "    photo_filenames = file.split(\"\\n\")[:-1]\n",
        "    return photo_filenames\n",
        "\n",
        "def load_cleaned_descriptions(filename, photo_filenames):\n",
        "    # Load cleaned descriptions\n",
        "    file = load_text_file(filename)\n",
        "    descriptions = {}\n",
        "\n",
        "    for line in file.split(\"\\n\"):\n",
        "        words = line.split()\n",
        "\n",
        "        if len(words) < 1:\n",
        "            continue\n",
        "\n",
        "        image_filename, image_caption = words[0], words[1:]\n",
        "\n",
        "        if image_filename in photo_filenames:\n",
        "            if image_filename not in descriptions:\n",
        "                descriptions[image_filename] = []\n",
        "\n",
        "            description = '<start> ' + \" \".join(image_caption) + ' <end>'\n",
        "            descriptions[image_filename].append(description)\n",
        "    return descriptions\n",
        "\n",
        "def load_selected_features(photo_filenames):\n",
        "    # Load all image features and select only the needed ones\n",
        "    all_image_features = load(open(\"image_features.p\", \"rb\"))\n",
        "    selected_features = {k: all_image_features[k] for k in photo_filenames}\n",
        "    return selected_features\n",
        "\n",
        "# Define the filename for training data\n",
        "training_data_filename = text_data_folder + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
        "\n",
        "# Load training data\n",
        "training_image_filenames = load_training_data(training_data_filename)\n",
        "training_descriptions = load_cleaned_descriptions(\"cleaned_descriptions.txt\",\n",
        "                                                  training_image_filenames)\n",
        "training_image_features = load_selected_features(training_image_filenames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSPdS9Jqf5j7"
      },
      "source": [
        "#Tokenizing the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qulGQNjWgZr6",
        "outputId": "9f2b9917-7ec6-40e7-beaa-4b0f59d1d9e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert a dictionary of descriptions into a flat list\n",
        "def descriptions_to_list(descriptions):\n",
        "    all_descriptions = []\n",
        "    for image_key in descriptions.keys():\n",
        "        [all_descriptions.append(description) for description in\n",
        "         descriptions[image_key]]\n",
        "    return all_descriptions\n",
        "\n",
        "# Create a text tokenizer to vectorize the text corpus\n",
        "# Each integer will represent a token in the dictionary\n",
        "def build_text_tokenizer(descriptions):\n",
        "    description_list = descriptions_to_list(descriptions)\n",
        "    tokenizer = Tokenizer()\n",
        "    tokenizer.fit_on_texts(description_list)\n",
        "    return tokenizer\n",
        "\n",
        "# Build a tokenizer and save it as a pickle file\n",
        "tokenizer = build_text_tokenizer(training_descriptions)\n",
        "dump(tokenizer, open('text_tokenizer.p', 'wb'))\n",
        "vocabulary_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "#Calculate the maximum length of descriptions\n",
        "def calculate_max_description_length(descriptions):\n",
        "    description_list = descriptions_to_list(descriptions)\n",
        "    return max(len(description.split()) for description in description_list)\n",
        "\n",
        "max_description_length = calculate_max_description_length(image_captions)\n",
        "max_description_length"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEQQPkF5gubN"
      },
      "source": [
        "#Building the CNN-RNN (LSTM) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNAvsCCgguHv"
      },
      "outputs": [],
      "source": [
        "# Define the image captioning model with additional layers\n",
        "def create_captioning_model(vocab_size, max_sequence_length):\n",
        "    # Define the input for image features\n",
        "    image_input = Input(shape=(2048,))\n",
        "    image_dropout = Dropout(0.5)(image_input)\n",
        "    image_fc1 = Dense(512, activation='relu')(image_dropout)  # Additional dense layer\n",
        "    image_fc2 = Dense(256, activation='relu')(image_fc1)\n",
        "\n",
        "    # Define the input for text sequences\n",
        "    text_input = Input(shape=(max_sequence_length,))\n",
        "    text_embed = Embedding(input_dim=vocab_size, output_dim=256, mask_zero=True)(text_input)\n",
        "    text_dropout = Dropout(0.5)(text_embed)\n",
        "    text_lstm1 = LSTM(256, return_sequences=True)(text_dropout)  # Additional LSTM layer\n",
        "    text_lstm2 = LSTM(256)(text_lstm1)\n",
        "\n",
        "    # Merge the image and text models\n",
        "    merged = add([image_fc2, text_lstm2])\n",
        "    merged_fc = Dense(256, activation='relu')(merged)\n",
        "    output = Dense(vocab_size, activation='softmax')(merged_fc)\n",
        "\n",
        "    # Create the model\n",
        "    model = Model(inputs=[image_input, text_input], outputs=output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "    # Summarize the model\n",
        "    plot_model(model, to_file='captioning_model.png', show_shapes=True)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JwYAmGgpQj"
      },
      "source": [
        "#Create Data generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIXvstzNgnRb"
      },
      "outputs": [],
      "source": [
        "# Create input-output sequence pairs from image descriptions\n",
        "def create_sequence_pairs(tokenizer, max_length, description_list, feature):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    # Process each description for the image\n",
        "    for description in description_list:\n",
        "        # Encode the text sequence\n",
        "        sequence = tokenizer.texts_to_sequences([description])[0]\n",
        "        # Split one sequence into multiple input-output pairs\n",
        "        for i in range(1, len(sequence)):\n",
        "            # Split into input and output pair\n",
        "            input_sequence, output_word = sequence[:i], sequence[i]\n",
        "            # Pad the input sequence\n",
        "            input_sequence = pad_sequences([input_sequence], maxlen=max_length)[0]\n",
        "            # Encode the output word\n",
        "            output_word = to_categorical([output_word], num_classes=vocabulary_size)[0]\n",
        "            # Store the pairs\n",
        "            X1.append(feature)\n",
        "            X2.append(input_sequence)\n",
        "            y.append(output_word)\n",
        "    return np.array(X1), np.array(X2), np.array(y)\n",
        "\n",
        "# Data generator used in model.fit_generator()\n",
        "def sequence_data_generator(descriptions, features, tokenizer, max_length):\n",
        "    while 1:\n",
        "        for image_key, description_list in descriptions.items():\n",
        "            # Retrieve image features\n",
        "            feature = features[image_key][0]\n",
        "            input_image, input_sequence, output_word = create_sequence_pairs(tokenizer, max_length, description_list, feature)\n",
        "            yield [[input_image, input_sequence], output_word]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbUF87mIg3Te"
      },
      "source": [
        "#Training the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wP6CAoIHg4sS"
      },
      "outputs": [],
      "source": [
        "# Create and compile the image captioning model\n",
        "model = create_captioning_model(vocabulary_size, max_description_length)\n",
        "num_epochs = 10\n",
        "num_steps = len(training_descriptions)\n",
        "\n",
        "# Create a directory 'models' to save the trained models\n",
        "os.mkdir(\"models\")\n",
        "\n",
        "# Train the model for the specified number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    data_gen = sequence_data_generator(training_descriptions, training_image_features, tokenizer, max_description_length)\n",
        "    model.fit_generator(data_gen, epochs=1, steps_per_epoch=num_steps, verbose=1)\n",
        "    model.save(\"models/model_\" + str(epoch) + \".h5\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9E41Ei65lyi"
      },
      "source": [
        "#Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZaFYEAF5qch"
      },
      "outputs": [],
      "source": [
        "# Extract features from an image using a pre-trained model\n",
        "def extract_image_features(filename, model):\n",
        "    try:\n",
        "        image = Image.open(filename)\n",
        "    except:\n",
        "        print(\"ERROR: Unable to open the image! Ensure that the image path and file extension are correct.\")\n",
        "\n",
        "    # Resize the image to a specific size\n",
        "    image = image.resize((299, 299))\n",
        "    image = np.array(image)\n",
        "\n",
        "    # Convert images with 4 channels to 3 channels\n",
        "    if image.shape[2] == 4:\n",
        "        image = image[..., :3]\n",
        "\n",
        "    # Prepare the image for model prediction\n",
        "    image = np.expand_dims(image, axis=0)\n",
        "    image = image / 127.5\n",
        "    image = image - 1.0\n",
        "\n",
        "    # Extract features from the image using the pre-trained model\n",
        "    features = model.predict(image)\n",
        "    return features\n",
        "\n",
        "# Map an integer back to a word using a tokenizer\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# Generate a description for an image using a trained model\n",
        "def generate_description(model, tokenizer, image_features, max_length):\n",
        "    in_text = 'start'\n",
        "    for i in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        pred = model.predict([image_features, sequence], verbose=0)\n",
        "        pred = np.argmax(pred)\n",
        "        word = word_for_id(pred, tokenizer)\n",
        "        if word is None:\n",
        "            break\n",
        "        in_text += ' ' + word\n",
        "        if word == 'end':\n",
        "            break\n",
        "    return in_text\n",
        "\n",
        "# Define the path to the image, maximum description length, tokenizer, model, and the pre-trained model\n",
        "image_path = '/content/10815824_2997e03d76.jpg'\n",
        "tokenizer = load(open(\"/content/text_tokenizer.p\",\"rb\"))\n",
        "model = load_model('/content/models/model_9.h5')\n",
        "\n",
        "# Extract features from the image\n",
        "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
        "photo_features = extract_image_features(image_path, xception_model)\n",
        "\n",
        "# Open the image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Generate a description for the image using the trained model and display it along with the image\n",
        "description = generate_description(model, tokenizer, photo_features, max_description_length)\n",
        "print(\"\\n\\n\")\n",
        "print(description)\n",
        "plt.imshow(image)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}